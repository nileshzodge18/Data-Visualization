import streamlit as st
import json
import pandas as pd
from langchain_core.prompts import ChatPromptTemplate
from .data_visualization import plot_graph
from utils.global_config_setup import glob_vars

# Generator for Streaming Tokens
def generate_response_tokens(prompt, content, input_data):
    """
    Generates response tokens from a given prompt and input data using a language model.
    Args:
        prompt (str): The initial prompt to generate the response.
        content (str): The content to be used in the system message.
        input_data (str): The input data to be used in the human message.
    Yields:
        str: The generated token from the language model response.
    """

    llm = glob_vars.llm
    prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",content
        ),
        (
            "human", "{input_data}"
        ),
        (
            "human", "{question_data}"
        ),
    ]
    )

    chain = prompt | llm

    response = ""
    for partial_resp in chain.stream(
    {
        "input_data": input_data,
        "output": "output",
        "question_data": prompt
    }
    ):
        token = partial_resp.content
        # glob_vars["fullMessage"] += token
        yield token


    # response = ollama.chat(model='llama3.2', stream=True, messages=glob_vars.messages)
    # for partial_resp in response:
    #     token = partial_resp["message"]["content"]
    #     glob_vars["fullMessage"] += token
    #     yield token

def fetch_llm_response(final_prompt_message, prompt,content,input_data) -> str:
    """
    Fetches the response from a language model based on the provided prompt and input data.
    This function appends the final prompt message to the global message list, generates response tokens
    from the language model, and updates the response in a markdown placeholder.
    Args:
        final_prompt_message (str): The final message to be appended to the global message list.
        prompt (str): The initial prompt to generate the response.
        content (str): Additional content to be used in generating the response.
        input_data (dict): Input data to be used in generating the response.
    Returns:
        str: The complete response generated by the language model.
    """

    response = ""
    markdown_placeholder = st.empty()
    for i in generate_response_tokens(prompt,content, input_data):
        response += i
        markdown_placeholder.markdown(response + "â–Œ")
    markdown_placeholder.empty()

    return response

def visualize_response(prompt, response, title_response, graph_type_response, input_data) -> None:
    """
    Visualizes the response from a prompt by generating a dataframe and plotting a graph.
    Parameters:
    prompt (str): The initial prompt given to the model.
    response (str): The response received from the model.
    title_response (str): The title for the graph.
    graph_type_response (str): The type of graph to be plotted.
    input_data (str): Additional input data for generating insights.
    Returns:
    None
    """

    try:
        if response.strip().startswith("{"):
            temp_obj = json.loads(response)
        else:
            import io
            response = response.replace("```csv\n", "").replace("\n```", "")
            temp_obj = pd.read_csv(io.StringIO(response))


    except Exception as e:
        st.write(e)
        return

    try:
        if not isinstance(temp_obj, pd.DataFrame):
            df = pd.DataFrame(temp_obj).reset_index(drop=True)
        else:
            df = temp_obj
    except Exception as e:
        st.write(e)
        return

    df = df.fillna(0)
    keys = list(df.keys())
    # st.write(f"Keys in Dataframe : {keys}")


    multi_bar = False

    # st.write(f"Total keys in Dataframe : {len(keys)}")

    if len(keys) > 2:
        multi_bar = True


    # st.markdown("<div style='border:2px solid black; padding: 10px; text-align: center;'><strong>Dataframe Table</strong></div>", unsafe_allow_html=True)
    st.write(f"\n\n")

    df_with_no_index = df.reset_index(drop=True)
    st.write(df_with_no_index)
    st.session_state.messages.append({"role": "assistant", "content": df_with_no_index})



    graph_type_response = graph_type_response.lower()



    plot_graph(df, keys, title_response, multi_bar, graph_type_response)

    
    # st.write("<h2>Conclusions and Insights</h2>", unsafe_allow_html=True)
    # glob_vars["fullMessage"] = response
    # glob_vars.messages = []
    # glob_vars.messages.append({"role": "user", "content": prompt})
    # glob_vars.messages.append({"role": "assistant", "content": glob_vars["fullMessage"]})
    # insightQuestion = "Provide some insights and summary for the response in the bullet points."
    # input_data = "User : " + prompt + "\n" + "Chatbot : " + str(df.to_dict())
    # # input_data = df.to_dict()
    # insight_system_content = (
    #     "You are a helpful assistant that return insights for {input_data} in bullet points. "
    #     "Please use less than 150 words for the insights. Give accurate information in formal language. "
    #     "Strictly don't return any programming code in the response.{input_data} contains the question and the response from Chatbot. "
    #     "Don't mention plotting of any graph or chart in the insights. "
    #     "Please examine chatbot response thoroughly and provide insights."
    # )
    # insight_system_ruthless = "PLEASE BE AS RUTHLESS AND RUDE AS POSSIBLE. ROAST THE DATA TO BEST OF YOUR ABILITY. ADD SOME INDIAN COMEDY AND SARCASM TO THE INSIGHTS. "
    # insight_system_comedy = "PLEASE BE AS FUNNY AS POSSIBLE. USE SOME COMEDY AND MAKE IT INTERESTING TO READ."
    # insight_system_interesting = "PLEASE BE AS INTERESTING AS POSSIBLE. MAKE IT INTERESTING TO READ SO THAT EVEN PEOPLE WHO DON'T READ FIND THIS INTERESTING."

    # insightResponse = fetchLlmResponse(insightQuestion, insightQuestion,insight_system_content,input_data)
    # st.write(insightResponse)
    return